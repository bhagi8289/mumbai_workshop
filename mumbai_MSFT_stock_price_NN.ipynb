{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mumbai_MSFT_stock_price_NN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jj9ruK1bd701",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f6b273e3-cb55-403d-a484-cfa8335484aa"
      },
      "source": [
        "# Standard Python Notebook Settings\n",
        "%config IPCompleter.greedy=True\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "from pprint import pprint\n",
        "import sklearn.datasets\n",
        "import ipywidgets as widgets\n",
        "from ipywidgets import interact, interact_manual\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sb\n",
        "import pandas as pd \n",
        "pd.option_context('display.max_rows', None, 'display.max_columns', None)\n",
        "import sys\n",
        "np.set_printoptions(threshold=sys.maxsize)\n",
        "np.set_printoptions(threshold=np.inf)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pandas._config.config.option_context at 0x7fb918f031d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKdU8ADHeQZL",
        "colab_type": "text"
      },
      "source": [
        "Code Source: https://towardsdatascience.com/neural-networks-to-predict-the-market-c4861b649371"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhdMXk3-fT69",
        "colab_type": "text"
      },
      "source": [
        "Source: https://learndatasci.com/tutorials/python-finance-part-yahoo-finance-api-pandas-matplotlib/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IClQ-5HueU6R",
        "colab_type": "code",
        "outputId": "a07a0db7-e03d-4d3d-8973-bd174ec1f25b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "!pip install pandas-datareader"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas-datareader in /usr/local/lib/python3.6/dist-packages (0.7.4)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (from pandas-datareader) (4.2.6)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.6/dist-packages (from pandas-datareader) (1.11.2)\n",
            "Requirement already satisfied: pandas>=0.19.2 in /usr/local/lib/python3.6/dist-packages (from pandas-datareader) (0.25.3)\n",
            "Requirement already satisfied: requests>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from pandas-datareader) (2.21.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.19.2->pandas-datareader) (1.17.4)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.19.2->pandas-datareader) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.19.2->pandas-datareader) (2.6.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->pandas-datareader) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->pandas-datareader) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->pandas-datareader) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->pandas-datareader) (2.8)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas>=0.19.2->pandas-datareader) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IC71KMpOfBz0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pandas_datareader import data\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGQEUp3Vflw6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the instruments to download. We would like to see Microsoft data.\n",
        "# We would like all available data from 01/01/2010 until 12/31/2019.\n",
        "start_date = '2010-01-01'\n",
        "end_date = '2019-12-31'\n",
        "\n",
        "# User pandas_reader.data.DataReader to load the desired data. As simple as that.\n",
        "panel_data = data.DataReader('MSFT', 'yahoo', start_date, end_date)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f58kZn4IgQnO",
        "colab_type": "code",
        "outputId": "451a9211-dc24-4a19-b8e8-61f6f218ec72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        }
      },
      "source": [
        "panel_data"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Open</th>\n",
              "      <th>Close</th>\n",
              "      <th>Volume</th>\n",
              "      <th>Adj Close</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2010-01-04</th>\n",
              "      <td>31.100000</td>\n",
              "      <td>30.590000</td>\n",
              "      <td>30.620001</td>\n",
              "      <td>30.950001</td>\n",
              "      <td>38409100.0</td>\n",
              "      <td>24.360727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-01-05</th>\n",
              "      <td>31.100000</td>\n",
              "      <td>30.639999</td>\n",
              "      <td>30.850000</td>\n",
              "      <td>30.959999</td>\n",
              "      <td>49749600.0</td>\n",
              "      <td>24.368599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-01-06</th>\n",
              "      <td>31.080000</td>\n",
              "      <td>30.520000</td>\n",
              "      <td>30.879999</td>\n",
              "      <td>30.770000</td>\n",
              "      <td>58182400.0</td>\n",
              "      <td>24.219046</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-01-07</th>\n",
              "      <td>30.700001</td>\n",
              "      <td>30.190001</td>\n",
              "      <td>30.629999</td>\n",
              "      <td>30.450001</td>\n",
              "      <td>50559700.0</td>\n",
              "      <td>23.967175</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-01-08</th>\n",
              "      <td>30.879999</td>\n",
              "      <td>30.240000</td>\n",
              "      <td>30.280001</td>\n",
              "      <td>30.660000</td>\n",
              "      <td>51197400.0</td>\n",
              "      <td>24.132469</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-12-24</th>\n",
              "      <td>157.710007</td>\n",
              "      <td>157.119995</td>\n",
              "      <td>157.479996</td>\n",
              "      <td>157.380005</td>\n",
              "      <td>8989200.0</td>\n",
              "      <td>157.380005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-12-26</th>\n",
              "      <td>158.729996</td>\n",
              "      <td>157.399994</td>\n",
              "      <td>157.559998</td>\n",
              "      <td>158.669998</td>\n",
              "      <td>14520600.0</td>\n",
              "      <td>158.669998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-12-27</th>\n",
              "      <td>159.550003</td>\n",
              "      <td>158.220001</td>\n",
              "      <td>159.449997</td>\n",
              "      <td>158.960007</td>\n",
              "      <td>18412800.0</td>\n",
              "      <td>158.960007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-12-30</th>\n",
              "      <td>159.020004</td>\n",
              "      <td>156.729996</td>\n",
              "      <td>158.990005</td>\n",
              "      <td>157.589996</td>\n",
              "      <td>16348400.0</td>\n",
              "      <td>157.589996</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-12-31</th>\n",
              "      <td>157.770004</td>\n",
              "      <td>156.449997</td>\n",
              "      <td>156.770004</td>\n",
              "      <td>157.699997</td>\n",
              "      <td>18369400.0</td>\n",
              "      <td>157.699997</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2516 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                  High         Low  ...      Volume   Adj Close\n",
              "Date                                ...                        \n",
              "2010-01-04   31.100000   30.590000  ...  38409100.0   24.360727\n",
              "2010-01-05   31.100000   30.639999  ...  49749600.0   24.368599\n",
              "2010-01-06   31.080000   30.520000  ...  58182400.0   24.219046\n",
              "2010-01-07   30.700001   30.190001  ...  50559700.0   23.967175\n",
              "2010-01-08   30.879999   30.240000  ...  51197400.0   24.132469\n",
              "...                ...         ...  ...         ...         ...\n",
              "2019-12-24  157.710007  157.119995  ...   8989200.0  157.380005\n",
              "2019-12-26  158.729996  157.399994  ...  14520600.0  158.669998\n",
              "2019-12-27  159.550003  158.220001  ...  18412800.0  158.960007\n",
              "2019-12-30  159.020004  156.729996  ...  16348400.0  157.589996\n",
              "2019-12-31  157.770004  156.449997  ...  18369400.0  157.699997\n",
              "\n",
              "[2516 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "of7VaqOKhQCe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "next_day_opening = panel_data['Open'].iloc[1:2516]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4h9iuoKbgWHS",
        "colab_type": "code",
        "outputId": "afb8cabd-6d9f-457b-f001-731af47fec59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "next_day_opening"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Date\n",
              "2010-01-05     30.850000\n",
              "2010-01-06     30.879999\n",
              "2010-01-07     30.629999\n",
              "2010-01-08     30.280001\n",
              "2010-01-11     30.709999\n",
              "                 ...    \n",
              "2019-12-24    157.479996\n",
              "2019-12-26    157.559998\n",
              "2019-12-27    159.449997\n",
              "2019-12-30    158.990005\n",
              "2019-12-31    156.770004\n",
              "Name: Open, Length: 2515, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEZt3wKii7wJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = panel_data[['High','Low',\t'Open',\t'Close','Volume','Adj Close']].iloc[0:2515]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvNh-a4UjpOn",
        "colab_type": "code",
        "outputId": "e20b3cbf-f29a-44f4-f468-71393f12cd54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "data.tail()"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Open</th>\n",
              "      <th>Close</th>\n",
              "      <th>Volume</th>\n",
              "      <th>Adj Close</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2019-12-23</th>\n",
              "      <td>158.119995</td>\n",
              "      <td>157.270004</td>\n",
              "      <td>158.119995</td>\n",
              "      <td>157.410004</td>\n",
              "      <td>17718200.0</td>\n",
              "      <td>157.410004</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-12-24</th>\n",
              "      <td>157.710007</td>\n",
              "      <td>157.119995</td>\n",
              "      <td>157.479996</td>\n",
              "      <td>157.380005</td>\n",
              "      <td>8989200.0</td>\n",
              "      <td>157.380005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-12-26</th>\n",
              "      <td>158.729996</td>\n",
              "      <td>157.399994</td>\n",
              "      <td>157.559998</td>\n",
              "      <td>158.669998</td>\n",
              "      <td>14520600.0</td>\n",
              "      <td>158.669998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-12-27</th>\n",
              "      <td>159.550003</td>\n",
              "      <td>158.220001</td>\n",
              "      <td>159.449997</td>\n",
              "      <td>158.960007</td>\n",
              "      <td>18412800.0</td>\n",
              "      <td>158.960007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-12-30</th>\n",
              "      <td>159.020004</td>\n",
              "      <td>156.729996</td>\n",
              "      <td>158.990005</td>\n",
              "      <td>157.589996</td>\n",
              "      <td>16348400.0</td>\n",
              "      <td>157.589996</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  High         Low  ...      Volume   Adj Close\n",
              "Date                                ...                        \n",
              "2019-12-23  158.119995  157.270004  ...  17718200.0  157.410004\n",
              "2019-12-24  157.710007  157.119995  ...   8989200.0  157.380005\n",
              "2019-12-26  158.729996  157.399994  ...  14520600.0  158.669998\n",
              "2019-12-27  159.550003  158.220001  ...  18412800.0  158.960007\n",
              "2019-12-30  159.020004  156.729996  ...  16348400.0  157.589996\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Lt1WwWHkJMd",
        "colab_type": "code",
        "outputId": "d4c34361-91a9-4d20-9390-c47b61d67392",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "next_day_opening"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Date\n",
              "2010-01-05     30.850000\n",
              "2010-01-06     30.879999\n",
              "2010-01-07     30.629999\n",
              "2010-01-08     30.280001\n",
              "2010-01-11     30.709999\n",
              "                 ...    \n",
              "2019-12-24    157.479996\n",
              "2019-12-26    157.559998\n",
              "2019-12-27    159.449997\n",
              "2019-12-30    158.990005\n",
              "2019-12-31    156.770004\n",
              "Name: Open, Length: 2515, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "077fe5IWkPRh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_np = data.to_numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcJLczAYkbO3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "next_np = next_day_opening.to_numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQ4vmc4BlCxl",
        "colab_type": "code",
        "outputId": "24f0ed3e-af32-4d88-cac5-fe112de38ab6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data_np.shape"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2515, 6)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vj9VYYcrlLk3",
        "colab_type": "code",
        "outputId": "b1c0517b-ad06-445f-d4f2-a3cc2ea3b35f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "next_np.shape"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2515,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESANwAsElxbk",
        "colab_type": "code",
        "outputId": "c4a877cb-263b-42ca-c204-4dcc38f23c1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "next_np.reshape(2515,1)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 30.85000038],\n",
              "       [ 30.87999916],\n",
              "       [ 30.62999916],\n",
              "       ...,\n",
              "       [159.44999695],\n",
              "       [158.99000549],\n",
              "       [156.77000427]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umM6lWATk62N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "np.set_printoptions(threshold=50)\n",
        "\n",
        "stock_data = np.column_stack([data_np,next_np])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGLWW43ymQze",
        "colab_type": "code",
        "outputId": "4970f33c-530d-473e-c2f9-4238dc046654",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "np.set_printoptions(suppress=True)\n",
        "stock_data[1]"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([      31.10000038,       30.63999939,       30.85000038,\n",
              "             30.95999908, 49749600.        ,       24.36859894,\n",
              "             30.87999916])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylSkusbTm6fz",
        "colab_type": "code",
        "outputId": "2120b68a-1aa4-4d38-f0c9-5912e304eb4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "stock_data.shape"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2515, 7)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfN3GoDAo2Jt",
        "colab_type": "code",
        "outputId": "828849d3-7e74-4960-fabe-692d874b6da6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X = stock_data[:,0:2]\n",
        "y = stock_data[:,6]\n",
        "print(X[1])\n",
        "print(y[1])"
      ],
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[31.10000038 30.63999939]\n",
            "30.8799991607666\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHVohjBJz-QJ",
        "colab_type": "code",
        "outputId": "63a1529f-8788-4df1-8d91-e40e1edae3a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X.shape"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2515, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUC_64DH0CEB",
        "colab_type": "code",
        "outputId": "238fb6ec-bbe9-4943-b63e-7f4e7296a35a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y.shape"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2515,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAzf2tgnIW6r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Scale the inputs\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "#scalerX = StandardScaler().fit(X)\n",
        "#y = y.reshape(-1, 1)\n",
        "#scalery = StandardScaler().fit(y)\n",
        "#X = scalerX.transform(X)\n",
        "#y = scalery.transform(y)\n",
        "#y = np.log(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaFfvQUkqFUB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.3, random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhAZdba_0BBw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxvJUTt3qxz6",
        "colab_type": "code",
        "outputId": "60f917ad-32df-4f48-f4dd-ea02bc642534",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(X_train.shape,\"\\t\",y_train.shape,\"\\n\",X_test.shape,\"\\t\",y_test.shape)"
      ],
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1760, 2) \t (1760,) \n",
            " (755, 2) \t (755,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYvt2j_n2dWY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error \n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sb\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings \n",
        "warnings.filterwarnings('ignore')\n",
        "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
        "from xgboost import XGBRegressor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9V4Nb7tr50e",
        "colab_type": "code",
        "outputId": "ce76dee9-ced9-49a1-a056-e98c0be29338",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "NN_model = Sequential()\n",
        "\n",
        "# The Input Layer :\n",
        "NN_model.add(Dense(128, kernel_initializer='normal',input_dim = X_train.shape[1], activation='relu'))\n",
        "\n",
        "# The Hidden Layers :\n",
        "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
        "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
        "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
        "\n",
        "# The Output Layer :\n",
        "NN_model.add(Dense(1, kernel_initializer='normal',activation='linear'))\n",
        "\n",
        "# Compile the network :\n",
        "NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
        "NN_model.summary()"
      ],
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_51 (Dense)             (None, 128)               384       \n",
            "_________________________________________________________________\n",
            "dense_52 (Dense)             (None, 256)               33024     \n",
            "_________________________________________________________________\n",
            "dense_53 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dense_54 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dense_55 (Dense)             (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 165,249\n",
            "Trainable params: 165,249\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FoeCUSBk2qxo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \n",
        "checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfAE1EwV2063",
        "colab_type": "code",
        "outputId": "830e02ce-bed1-451b-8060-f52fc6e99b5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Train the model\n",
        "NN_model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split = 0.2, callbacks=callbacks_list)"
      ],
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1408 samples, validate on 352 samples\n",
            "Epoch 1/100\n",
            "1408/1408 [==============================] - 1s 766us/step - loss: 15.2677 - mean_absolute_error: 15.2677 - val_loss: 1.4016 - val_mean_absolute_error: 1.4016\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.40162, saving model to Weights-001--1.40162.hdf5\n",
            "Epoch 2/100\n",
            "1408/1408 [==============================] - 0s 127us/step - loss: 0.5918 - mean_absolute_error: 0.5918 - val_loss: 0.4574 - val_mean_absolute_error: 0.4574\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.40162 to 0.45737, saving model to Weights-002--0.45737.hdf5\n",
            "Epoch 3/100\n",
            "1408/1408 [==============================] - 0s 122us/step - loss: 0.9376 - mean_absolute_error: 0.9376 - val_loss: 0.5493 - val_mean_absolute_error: 0.5493\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.45737\n",
            "Epoch 4/100\n",
            "1408/1408 [==============================] - 0s 127us/step - loss: 0.9723 - mean_absolute_error: 0.9723 - val_loss: 1.0319 - val_mean_absolute_error: 1.0319\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.45737\n",
            "Epoch 5/100\n",
            "1408/1408 [==============================] - 0s 127us/step - loss: 0.9703 - mean_absolute_error: 0.9703 - val_loss: 1.2041 - val_mean_absolute_error: 1.2041\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.45737\n",
            "Epoch 6/100\n",
            "1408/1408 [==============================] - 0s 126us/step - loss: 0.6173 - mean_absolute_error: 0.6173 - val_loss: 1.1005 - val_mean_absolute_error: 1.1005\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.45737\n",
            "Epoch 7/100\n",
            "1408/1408 [==============================] - 0s 122us/step - loss: 0.6321 - mean_absolute_error: 0.6321 - val_loss: 0.5446 - val_mean_absolute_error: 0.5446\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.45737\n",
            "Epoch 8/100\n",
            "1408/1408 [==============================] - 0s 125us/step - loss: 1.1636 - mean_absolute_error: 1.1636 - val_loss: 0.6832 - val_mean_absolute_error: 0.6832\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.45737\n",
            "Epoch 9/100\n",
            "1408/1408 [==============================] - 0s 122us/step - loss: 0.5653 - mean_absolute_error: 0.5653 - val_loss: 1.2933 - val_mean_absolute_error: 1.2933\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.45737\n",
            "Epoch 10/100\n",
            "1408/1408 [==============================] - 0s 127us/step - loss: 1.2913 - mean_absolute_error: 1.2913 - val_loss: 0.7479 - val_mean_absolute_error: 0.7479\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.45737\n",
            "Epoch 11/100\n",
            "1408/1408 [==============================] - 0s 133us/step - loss: 1.6606 - mean_absolute_error: 1.6606 - val_loss: 0.6413 - val_mean_absolute_error: 0.6413\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.45737\n",
            "Epoch 12/100\n",
            "1408/1408 [==============================] - 0s 127us/step - loss: 0.8865 - mean_absolute_error: 0.8865 - val_loss: 0.5504 - val_mean_absolute_error: 0.5504\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.45737\n",
            "Epoch 13/100\n",
            "1408/1408 [==============================] - 0s 132us/step - loss: 0.7456 - mean_absolute_error: 0.7456 - val_loss: 0.5724 - val_mean_absolute_error: 0.5724\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.45737\n",
            "Epoch 14/100\n",
            "1408/1408 [==============================] - 0s 121us/step - loss: 0.5507 - mean_absolute_error: 0.5507 - val_loss: 0.4422 - val_mean_absolute_error: 0.4422\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.45737 to 0.44215, saving model to Weights-014--0.44215.hdf5\n",
            "Epoch 15/100\n",
            "1408/1408 [==============================] - 0s 127us/step - loss: 0.5524 - mean_absolute_error: 0.5524 - val_loss: 0.4428 - val_mean_absolute_error: 0.4428\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.44215\n",
            "Epoch 16/100\n",
            "1408/1408 [==============================] - 0s 123us/step - loss: 1.0303 - mean_absolute_error: 1.0303 - val_loss: 0.4392 - val_mean_absolute_error: 0.4392\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.44215 to 0.43916, saving model to Weights-016--0.43916.hdf5\n",
            "Epoch 17/100\n",
            "1408/1408 [==============================] - 0s 125us/step - loss: 0.5088 - mean_absolute_error: 0.5088 - val_loss: 0.4528 - val_mean_absolute_error: 0.4528\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.43916\n",
            "Epoch 18/100\n",
            "1408/1408 [==============================] - 0s 119us/step - loss: 0.9770 - mean_absolute_error: 0.9770 - val_loss: 1.2960 - val_mean_absolute_error: 1.2960\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.43916\n",
            "Epoch 19/100\n",
            "1408/1408 [==============================] - 0s 131us/step - loss: 1.1056 - mean_absolute_error: 1.1056 - val_loss: 0.6831 - val_mean_absolute_error: 0.6831\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.43916\n",
            "Epoch 20/100\n",
            "1408/1408 [==============================] - 0s 129us/step - loss: 0.8203 - mean_absolute_error: 0.8203 - val_loss: 0.6449 - val_mean_absolute_error: 0.6449\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.43916\n",
            "Epoch 21/100\n",
            "1408/1408 [==============================] - 0s 130us/step - loss: 0.9663 - mean_absolute_error: 0.9663 - val_loss: 1.0431 - val_mean_absolute_error: 1.0431\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.43916\n",
            "Epoch 22/100\n",
            "1408/1408 [==============================] - 0s 129us/step - loss: 0.7569 - mean_absolute_error: 0.7569 - val_loss: 0.8229 - val_mean_absolute_error: 0.8229\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.43916\n",
            "Epoch 23/100\n",
            "1408/1408 [==============================] - 0s 129us/step - loss: 0.6140 - mean_absolute_error: 0.6140 - val_loss: 0.5878 - val_mean_absolute_error: 0.5878\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.43916\n",
            "Epoch 24/100\n",
            "1408/1408 [==============================] - 0s 120us/step - loss: 0.6510 - mean_absolute_error: 0.6510 - val_loss: 0.7447 - val_mean_absolute_error: 0.7447\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.43916\n",
            "Epoch 25/100\n",
            "1408/1408 [==============================] - 0s 124us/step - loss: 0.5047 - mean_absolute_error: 0.5047 - val_loss: 0.6307 - val_mean_absolute_error: 0.6307\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.43916\n",
            "Epoch 26/100\n",
            "1408/1408 [==============================] - 0s 127us/step - loss: 0.6763 - mean_absolute_error: 0.6763 - val_loss: 0.9162 - val_mean_absolute_error: 0.9162\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.43916\n",
            "Epoch 27/100\n",
            "1408/1408 [==============================] - 0s 121us/step - loss: 0.9483 - mean_absolute_error: 0.9483 - val_loss: 1.7772 - val_mean_absolute_error: 1.7772\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.43916\n",
            "Epoch 28/100\n",
            "1408/1408 [==============================] - 0s 119us/step - loss: 1.0587 - mean_absolute_error: 1.0587 - val_loss: 0.5238 - val_mean_absolute_error: 0.5238\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.43916\n",
            "Epoch 29/100\n",
            "1408/1408 [==============================] - 0s 118us/step - loss: 0.5852 - mean_absolute_error: 0.5852 - val_loss: 0.5891 - val_mean_absolute_error: 0.5891\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.43916\n",
            "Epoch 30/100\n",
            "1408/1408 [==============================] - 0s 131us/step - loss: 0.5184 - mean_absolute_error: 0.5184 - val_loss: 0.5602 - val_mean_absolute_error: 0.5602\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.43916\n",
            "Epoch 31/100\n",
            "1408/1408 [==============================] - 0s 125us/step - loss: 0.6682 - mean_absolute_error: 0.6682 - val_loss: 1.2703 - val_mean_absolute_error: 1.2703\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.43916\n",
            "Epoch 32/100\n",
            "1408/1408 [==============================] - 0s 132us/step - loss: 0.7030 - mean_absolute_error: 0.7030 - val_loss: 1.5712 - val_mean_absolute_error: 1.5712\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.43916\n",
            "Epoch 33/100\n",
            "1408/1408 [==============================] - 0s 128us/step - loss: 0.7330 - mean_absolute_error: 0.7330 - val_loss: 1.2147 - val_mean_absolute_error: 1.2147\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.43916\n",
            "Epoch 34/100\n",
            "1408/1408 [==============================] - 0s 146us/step - loss: 0.8544 - mean_absolute_error: 0.8544 - val_loss: 0.4423 - val_mean_absolute_error: 0.4423\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.43916\n",
            "Epoch 35/100\n",
            "1408/1408 [==============================] - 0s 133us/step - loss: 0.6305 - mean_absolute_error: 0.6305 - val_loss: 1.0173 - val_mean_absolute_error: 1.0173\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.43916\n",
            "Epoch 36/100\n",
            "1408/1408 [==============================] - 0s 138us/step - loss: 0.5460 - mean_absolute_error: 0.5460 - val_loss: 1.4910 - val_mean_absolute_error: 1.4910\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.43916\n",
            "Epoch 37/100\n",
            "1408/1408 [==============================] - 0s 148us/step - loss: 1.1967 - mean_absolute_error: 1.1967 - val_loss: 0.5979 - val_mean_absolute_error: 0.5979\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.43916\n",
            "Epoch 38/100\n",
            "1408/1408 [==============================] - 0s 158us/step - loss: 0.8151 - mean_absolute_error: 0.8151 - val_loss: 2.2424 - val_mean_absolute_error: 2.2424\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.43916\n",
            "Epoch 39/100\n",
            "1408/1408 [==============================] - 0s 153us/step - loss: 0.6937 - mean_absolute_error: 0.6937 - val_loss: 0.6984 - val_mean_absolute_error: 0.6984\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.43916\n",
            "Epoch 40/100\n",
            "1408/1408 [==============================] - 0s 152us/step - loss: 0.8874 - mean_absolute_error: 0.8874 - val_loss: 0.4852 - val_mean_absolute_error: 0.4852\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.43916\n",
            "Epoch 41/100\n",
            "1408/1408 [==============================] - 0s 148us/step - loss: 0.6802 - mean_absolute_error: 0.6802 - val_loss: 0.4795 - val_mean_absolute_error: 0.4795\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.43916\n",
            "Epoch 42/100\n",
            "1408/1408 [==============================] - 0s 147us/step - loss: 0.6294 - mean_absolute_error: 0.6294 - val_loss: 0.4877 - val_mean_absolute_error: 0.4877\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.43916\n",
            "Epoch 43/100\n",
            "1408/1408 [==============================] - 0s 150us/step - loss: 0.5583 - mean_absolute_error: 0.5583 - val_loss: 1.2359 - val_mean_absolute_error: 1.2359\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.43916\n",
            "Epoch 44/100\n",
            "1408/1408 [==============================] - 0s 146us/step - loss: 0.8030 - mean_absolute_error: 0.8030 - val_loss: 0.4435 - val_mean_absolute_error: 0.4435\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.43916\n",
            "Epoch 45/100\n",
            "1408/1408 [==============================] - 0s 144us/step - loss: 0.5221 - mean_absolute_error: 0.5221 - val_loss: 1.3441 - val_mean_absolute_error: 1.3441\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.43916\n",
            "Epoch 46/100\n",
            "1408/1408 [==============================] - 0s 146us/step - loss: 0.9171 - mean_absolute_error: 0.9171 - val_loss: 0.5577 - val_mean_absolute_error: 0.5577\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.43916\n",
            "Epoch 47/100\n",
            "1408/1408 [==============================] - 0s 150us/step - loss: 0.6006 - mean_absolute_error: 0.6006 - val_loss: 0.6529 - val_mean_absolute_error: 0.6529\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.43916\n",
            "Epoch 48/100\n",
            "1408/1408 [==============================] - 0s 146us/step - loss: 0.6984 - mean_absolute_error: 0.6984 - val_loss: 0.5228 - val_mean_absolute_error: 0.5228\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.43916\n",
            "Epoch 49/100\n",
            "1408/1408 [==============================] - 0s 148us/step - loss: 0.7450 - mean_absolute_error: 0.7450 - val_loss: 0.4968 - val_mean_absolute_error: 0.4968\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.43916\n",
            "Epoch 50/100\n",
            "1408/1408 [==============================] - 0s 150us/step - loss: 0.9743 - mean_absolute_error: 0.9743 - val_loss: 0.4688 - val_mean_absolute_error: 0.4688\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.43916\n",
            "Epoch 51/100\n",
            "1408/1408 [==============================] - 0s 152us/step - loss: 0.4511 - mean_absolute_error: 0.4511 - val_loss: 0.4597 - val_mean_absolute_error: 0.4597\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.43916\n",
            "Epoch 52/100\n",
            "1408/1408 [==============================] - 0s 169us/step - loss: 0.4886 - mean_absolute_error: 0.4886 - val_loss: 0.5163 - val_mean_absolute_error: 0.5163\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.43916\n",
            "Epoch 53/100\n",
            "1408/1408 [==============================] - 0s 155us/step - loss: 0.8052 - mean_absolute_error: 0.8052 - val_loss: 0.4392 - val_mean_absolute_error: 0.4392\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.43916\n",
            "Epoch 54/100\n",
            "1408/1408 [==============================] - 0s 152us/step - loss: 0.8863 - mean_absolute_error: 0.8863 - val_loss: 1.5031 - val_mean_absolute_error: 1.5031\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.43916\n",
            "Epoch 55/100\n",
            "1408/1408 [==============================] - 0s 155us/step - loss: 1.0648 - mean_absolute_error: 1.0648 - val_loss: 0.4668 - val_mean_absolute_error: 0.4668\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.43916\n",
            "Epoch 56/100\n",
            "1408/1408 [==============================] - 0s 144us/step - loss: 0.7622 - mean_absolute_error: 0.7622 - val_loss: 1.6309 - val_mean_absolute_error: 1.6309\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.43916\n",
            "Epoch 57/100\n",
            "1408/1408 [==============================] - 0s 138us/step - loss: 0.8177 - mean_absolute_error: 0.8177 - val_loss: 0.5655 - val_mean_absolute_error: 0.5655\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.43916\n",
            "Epoch 58/100\n",
            "1408/1408 [==============================] - 0s 147us/step - loss: 0.5987 - mean_absolute_error: 0.5987 - val_loss: 0.7171 - val_mean_absolute_error: 0.7171\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.43916\n",
            "Epoch 59/100\n",
            "1408/1408 [==============================] - 0s 148us/step - loss: 0.7118 - mean_absolute_error: 0.7118 - val_loss: 0.4935 - val_mean_absolute_error: 0.4935\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.43916\n",
            "Epoch 60/100\n",
            "1408/1408 [==============================] - 0s 152us/step - loss: 0.7033 - mean_absolute_error: 0.7033 - val_loss: 0.4558 - val_mean_absolute_error: 0.4558\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.43916\n",
            "Epoch 61/100\n",
            "1408/1408 [==============================] - 0s 143us/step - loss: 0.7445 - mean_absolute_error: 0.7445 - val_loss: 2.6579 - val_mean_absolute_error: 2.6579\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.43916\n",
            "Epoch 62/100\n",
            "1408/1408 [==============================] - 0s 158us/step - loss: 0.8143 - mean_absolute_error: 0.8143 - val_loss: 0.4446 - val_mean_absolute_error: 0.4446\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.43916\n",
            "Epoch 63/100\n",
            "1408/1408 [==============================] - 0s 151us/step - loss: 0.6680 - mean_absolute_error: 0.6680 - val_loss: 0.8615 - val_mean_absolute_error: 0.8615\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.43916\n",
            "Epoch 64/100\n",
            "1408/1408 [==============================] - 0s 150us/step - loss: 0.7136 - mean_absolute_error: 0.7136 - val_loss: 0.4407 - val_mean_absolute_error: 0.4407\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.43916\n",
            "Epoch 65/100\n",
            "1408/1408 [==============================] - 0s 143us/step - loss: 0.5635 - mean_absolute_error: 0.5635 - val_loss: 0.5677 - val_mean_absolute_error: 0.5677\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.43916\n",
            "Epoch 66/100\n",
            "1408/1408 [==============================] - 0s 151us/step - loss: 0.5402 - mean_absolute_error: 0.5402 - val_loss: 0.4448 - val_mean_absolute_error: 0.4448\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.43916\n",
            "Epoch 67/100\n",
            "1408/1408 [==============================] - 0s 149us/step - loss: 0.6084 - mean_absolute_error: 0.6084 - val_loss: 0.6694 - val_mean_absolute_error: 0.6694\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.43916\n",
            "Epoch 68/100\n",
            "1408/1408 [==============================] - 0s 151us/step - loss: 0.6867 - mean_absolute_error: 0.6867 - val_loss: 0.4649 - val_mean_absolute_error: 0.4649\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.43916\n",
            "Epoch 69/100\n",
            "1408/1408 [==============================] - 0s 153us/step - loss: 1.3265 - mean_absolute_error: 1.3265 - val_loss: 0.4554 - val_mean_absolute_error: 0.4554\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.43916\n",
            "Epoch 70/100\n",
            "1408/1408 [==============================] - 0s 139us/step - loss: 0.6275 - mean_absolute_error: 0.6275 - val_loss: 0.7612 - val_mean_absolute_error: 0.7612\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.43916\n",
            "Epoch 71/100\n",
            "1408/1408 [==============================] - 0s 157us/step - loss: 0.6320 - mean_absolute_error: 0.6320 - val_loss: 0.9499 - val_mean_absolute_error: 0.9499\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.43916\n",
            "Epoch 72/100\n",
            "1408/1408 [==============================] - 0s 148us/step - loss: 0.5386 - mean_absolute_error: 0.5386 - val_loss: 0.5193 - val_mean_absolute_error: 0.5193\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.43916\n",
            "Epoch 73/100\n",
            "1408/1408 [==============================] - 0s 156us/step - loss: 0.6877 - mean_absolute_error: 0.6877 - val_loss: 0.8055 - val_mean_absolute_error: 0.8055\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.43916\n",
            "Epoch 74/100\n",
            "1408/1408 [==============================] - 0s 167us/step - loss: 0.8655 - mean_absolute_error: 0.8655 - val_loss: 0.7498 - val_mean_absolute_error: 0.7498\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.43916\n",
            "Epoch 75/100\n",
            "1408/1408 [==============================] - 0s 157us/step - loss: 0.5725 - mean_absolute_error: 0.5725 - val_loss: 0.6051 - val_mean_absolute_error: 0.6051\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.43916\n",
            "Epoch 76/100\n",
            "1408/1408 [==============================] - 0s 140us/step - loss: 0.6315 - mean_absolute_error: 0.6315 - val_loss: 0.4871 - val_mean_absolute_error: 0.4871\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.43916\n",
            "Epoch 77/100\n",
            "1408/1408 [==============================] - 0s 147us/step - loss: 0.5827 - mean_absolute_error: 0.5827 - val_loss: 0.5756 - val_mean_absolute_error: 0.5756\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.43916\n",
            "Epoch 78/100\n",
            "1408/1408 [==============================] - 0s 149us/step - loss: 0.6149 - mean_absolute_error: 0.6149 - val_loss: 0.8892 - val_mean_absolute_error: 0.8892\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.43916\n",
            "Epoch 79/100\n",
            "1408/1408 [==============================] - 0s 141us/step - loss: 0.6776 - mean_absolute_error: 0.6776 - val_loss: 0.7601 - val_mean_absolute_error: 0.7601\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.43916\n",
            "Epoch 80/100\n",
            "1408/1408 [==============================] - 0s 137us/step - loss: 0.6281 - mean_absolute_error: 0.6281 - val_loss: 0.6521 - val_mean_absolute_error: 0.6521\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.43916\n",
            "Epoch 81/100\n",
            "1408/1408 [==============================] - 0s 142us/step - loss: 0.6509 - mean_absolute_error: 0.6509 - val_loss: 0.5058 - val_mean_absolute_error: 0.5058\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.43916\n",
            "Epoch 82/100\n",
            "1408/1408 [==============================] - 0s 150us/step - loss: 0.8531 - mean_absolute_error: 0.8531 - val_loss: 0.5428 - val_mean_absolute_error: 0.5428\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.43916\n",
            "Epoch 83/100\n",
            "1408/1408 [==============================] - 0s 148us/step - loss: 0.6692 - mean_absolute_error: 0.6692 - val_loss: 0.7688 - val_mean_absolute_error: 0.7688\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.43916\n",
            "Epoch 84/100\n",
            "1408/1408 [==============================] - 0s 140us/step - loss: 0.6952 - mean_absolute_error: 0.6952 - val_loss: 0.4678 - val_mean_absolute_error: 0.4678\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.43916\n",
            "Epoch 85/100\n",
            "1408/1408 [==============================] - 0s 148us/step - loss: 0.7048 - mean_absolute_error: 0.7048 - val_loss: 0.5255 - val_mean_absolute_error: 0.5255\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.43916\n",
            "Epoch 86/100\n",
            "1408/1408 [==============================] - 0s 143us/step - loss: 0.8098 - mean_absolute_error: 0.8098 - val_loss: 0.4468 - val_mean_absolute_error: 0.4468\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.43916\n",
            "Epoch 87/100\n",
            "1408/1408 [==============================] - 0s 147us/step - loss: 0.5463 - mean_absolute_error: 0.5463 - val_loss: 0.4908 - val_mean_absolute_error: 0.4908\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.43916\n",
            "Epoch 88/100\n",
            "1408/1408 [==============================] - 0s 145us/step - loss: 0.8478 - mean_absolute_error: 0.8478 - val_loss: 1.6592 - val_mean_absolute_error: 1.6592\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.43916\n",
            "Epoch 89/100\n",
            "1408/1408 [==============================] - 0s 150us/step - loss: 1.3355 - mean_absolute_error: 1.3355 - val_loss: 3.6049 - val_mean_absolute_error: 3.6049\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.43916\n",
            "Epoch 90/100\n",
            "1408/1408 [==============================] - 0s 144us/step - loss: 0.7654 - mean_absolute_error: 0.7654 - val_loss: 0.4817 - val_mean_absolute_error: 0.4817\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.43916\n",
            "Epoch 91/100\n",
            "1408/1408 [==============================] - 0s 139us/step - loss: 0.5385 - mean_absolute_error: 0.5385 - val_loss: 0.5646 - val_mean_absolute_error: 0.5646\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.43916\n",
            "Epoch 92/100\n",
            "1408/1408 [==============================] - 0s 146us/step - loss: 0.5454 - mean_absolute_error: 0.5454 - val_loss: 0.5737 - val_mean_absolute_error: 0.5737\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.43916\n",
            "Epoch 93/100\n",
            "1408/1408 [==============================] - 0s 140us/step - loss: 0.7850 - mean_absolute_error: 0.7850 - val_loss: 1.1069 - val_mean_absolute_error: 1.1069\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.43916\n",
            "Epoch 94/100\n",
            "1408/1408 [==============================] - 0s 148us/step - loss: 0.7634 - mean_absolute_error: 0.7634 - val_loss: 1.0865 - val_mean_absolute_error: 1.0865\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.43916\n",
            "Epoch 95/100\n",
            "1408/1408 [==============================] - 0s 143us/step - loss: 0.7869 - mean_absolute_error: 0.7869 - val_loss: 0.5043 - val_mean_absolute_error: 0.5043\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.43916\n",
            "Epoch 96/100\n",
            "1408/1408 [==============================] - 0s 140us/step - loss: 0.6184 - mean_absolute_error: 0.6184 - val_loss: 0.6481 - val_mean_absolute_error: 0.6481\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.43916\n",
            "Epoch 97/100\n",
            "1408/1408 [==============================] - 0s 144us/step - loss: 0.5810 - mean_absolute_error: 0.5810 - val_loss: 0.4706 - val_mean_absolute_error: 0.4706\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.43916\n",
            "Epoch 98/100\n",
            "1408/1408 [==============================] - 0s 148us/step - loss: 0.5410 - mean_absolute_error: 0.5410 - val_loss: 0.4543 - val_mean_absolute_error: 0.4543\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.43916\n",
            "Epoch 99/100\n",
            "1408/1408 [==============================] - 0s 139us/step - loss: 0.8534 - mean_absolute_error: 0.8534 - val_loss: 0.5751 - val_mean_absolute_error: 0.5751\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.43916\n",
            "Epoch 100/100\n",
            "1408/1408 [==============================] - 0s 145us/step - loss: 0.5681 - mean_absolute_error: 0.5681 - val_loss: 0.9570 - val_mean_absolute_error: 0.9570\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.43916\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb8b6e68588>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 168
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjQ0KgegcH2u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "2359d38e-47ce-489c-db09-a4f1ffc133cd"
      },
      "source": [
        "!ls -ltr\n"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 3976\n",
            "drwxr-xr-x 1 root root    4096 Dec 18 16:52 sample_data\n",
            "-rw-r--r-- 1 root root 1353824 Jan  8 09:26 Weights-001--0.10046.hdf5\n",
            "-rw-r--r-- 1 root root 1353824 Jan  8 09:26 Weights-002--0.08434.hdf5\n",
            "-rw-r--r-- 1 root root 1353824 Jan  8 09:26 Weights-003--0.04438.hdf5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCoDh0-628Gw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load wights file of the best model :\n",
        "wights_file = 'Weights-016--0.43916.hdf5' # choose the best checkpoint \n",
        "NN_model.load_weights(wights_file) # load it\n",
        "NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tFvVNZW3WFR",
        "colab_type": "code",
        "outputId": "258472aa-e689-48ea-eefa-cf080c13758a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "NN_model.evaluate(X_test,y_test)"
      ],
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "755/755 [==============================] - 1s 1ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.36306736161377257, 0.36306736161377257]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 170
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5twiTw9FgTyn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "67831bf9-ee43-443a-8c55-be9d71bef18a"
      },
      "source": [
        "X_test[5]"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.19457648, 1.21869505, 1.21515125, 1.20900894])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ix6Dbfg-j5H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_new = np.array([159,\t156]).reshape(1,-1)\n",
        "#X_new = scalerX.transform(X_new)\t\n",
        "y_new = NN_model.predict(X_new)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMVHZeTEgkmB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9e5fa647-2bf6-4ced-82c6-2decdd9962a4"
      },
      "source": [
        "y_new\n"
      ],
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[157.97545]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 173
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDCTYWRri-a1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}