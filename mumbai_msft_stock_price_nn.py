# -*- coding: utf-8 -*-
"""mumbai_MSFT_stock_price_NN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1u5bz593oSYMW_xlVRQs8BAr9iatFOTFH
"""

# Commented out IPython magic to ensure Python compatibility.
# Standard Python Notebook Settings
# %config IPCompleter.greedy=True
from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity = "all"
from pprint import pprint
import sklearn.datasets
import ipywidgets as widgets
from ipywidgets import interact, interact_manual
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sb
import pandas as pd 
pd.option_context('display.max_rows', None, 'display.max_columns', None)
import sys
np.set_printoptions(threshold=sys.maxsize)
np.set_printoptions(threshold=np.inf)

"""Code Source: https://towardsdatascience.com/neural-networks-to-predict-the-market-c4861b649371

Source: https://learndatasci.com/tutorials/python-finance-part-yahoo-finance-api-pandas-matplotlib/
"""

!pip install pandas-datareader

from pandas_datareader import data
import matplotlib.pyplot as plt
import pandas as pd

# Define the instruments to download. We would like to see Microsoft data.
# We would like all available data from 01/01/2010 until 12/31/2019.
start_date = '2010-01-01'
end_date = '2019-12-31'

# User pandas_reader.data.DataReader to load the desired data. As simple as that.
panel_data = data.DataReader('MSFT', 'yahoo', start_date, end_date)

panel_data

next_day_opening = panel_data['Open'].iloc[1:2516]

next_day_opening

data = panel_data[['High','Low',	'Open',	'Close','Volume','Adj Close']].iloc[0:2515]

data.tail()

next_day_opening

data_np = data.to_numpy()

next_np = next_day_opening.to_numpy()

data_np.shape

next_np.shape

next_np.reshape(2515,1)

np.set_printoptions(threshold=50)

stock_data = np.column_stack([data_np,next_np])

np.set_printoptions(suppress=True)
stock_data[1]

stock_data.shape

from sklearn.model_selection import train_test_split
X = stock_data[:,0:2]
y = stock_data[:,6]
print(X[1])
print(y[1])

X.shape

y.shape

#Scale the inputs
from sklearn.preprocessing import StandardScaler
#scalerX = StandardScaler().fit(X)
#y = y.reshape(-1, 1)
#scalery = StandardScaler().fit(y)
#X = scalerX.transform(X)
#y = scalery.transform(y)
#y = np.log(y)

X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.3, random_state=0)



print(X_train.shape,"\t",y_train.shape,"\n",X_test.shape,"\t",y_test.shape)

from keras.callbacks import ModelCheckpoint
from keras.models import Sequential
from keras.layers import Dense, Activation, Flatten
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error 
from matplotlib import pyplot as plt
import seaborn as sb
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import warnings 
warnings.filterwarnings('ignore')
warnings.filterwarnings('ignore', category=DeprecationWarning)
from xgboost import XGBRegressor

NN_model = Sequential()

# The Input Layer :
NN_model.add(Dense(128, kernel_initializer='normal',input_dim = X_train.shape[1], activation='relu'))

# The Hidden Layers :
NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))
NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))
NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))

# The Output Layer :
NN_model.add(Dense(1, kernel_initializer='normal',activation='linear'))

# Compile the network :
NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])
NN_model.summary()

checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' 
checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')
callbacks_list = [checkpoint]

# Train the model
NN_model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split = 0.2, callbacks=callbacks_list)

!ls -ltr

# Load wights file of the best model :
wights_file = 'Weights-016--0.43916.hdf5' # choose the best checkpoint 
NN_model.load_weights(wights_file) # load it
NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])

NN_model.evaluate(X_test,y_test)

X_test[5]

X_new = np.array([159,	156]).reshape(1,-1)
#X_new = scalerX.transform(X_new)	
y_new = NN_model.predict(X_new)

y_new

