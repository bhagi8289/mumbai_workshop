# -*- coding: utf-8 -*-
"""mumbai_customer_churn_classification

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wP6m_vd2-_rQ2nPne2ixajV_-li4Ap-f

Source: http://blog.yhat.com/posts/predicting-customer-churn-with-sklearn.html


"Churn Rate" is a business term describing the rate at which customers leave or cease paying for a product or service. It's a critical figure in many businesses, as it's often the case that acquiring new customers is a lot more costly than retaining existing ones (in some cases, 5 to 20 times more expensive).

Understanding what keeps customers engaged, therefore, is incredibly valuable, as it is a logical foundation from which to develop retention strategies and roll out operational practices aimed to keep customers from walking out the door. Consequently, there's growing interest among companies to develop better churn-detection techniques, leading many to look to data mining and machine learning for new and creative approaches.

Predicting churn is particularly important for businesses w/ subscription models such as cell phone, cable, or merchant credit card processing plans. But modeling churn has wide reaching applications in many domains. For example, casinos have used predictive models to predict ideal room conditions for keeping patrons at the blackjack table and when to reward unlucky gamblers with front row seats to Celine Dion. Similarly, airlines may offer first class upgrades to complaining customers. The list goes on.

This is a post about modeling customer churn using Python.
"""

#Code Source: https://towardsdatascience.com/churn-prediction-770d6cb582a5

# Commented out IPython magic to ensure Python compatibility.
# Standard Python Notebook Settings
# %config IPCompleter.greedy=True
from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity = "all"
from pprint import pprint
import sklearn.datasets
import ipywidgets as widgets
from ipywidgets import interact, interact_manual
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sb
import pandas as pd 
pd.option_context('display.max_rows', None, 'display.max_columns', None)
import sys
np.set_printoptions(threshold=sys.maxsize)
np.set_printoptions(threshold=np.inf)

df = pd.read_csv('https://github.com/bhagi8289/mumbai_workshop/raw/master/churn.csv')
df.sample(5)

df.isnull().count()

df.describe().transpose()

df.describe(include=['object'])

df.notnull().count()

#df = df.dropna(how="all")

df1 = df.copy()

df1 = df1[~df1.duplicated()]

df1.count()

df1.dtypes

target = 'churn'
numerical_features= ['accountlength','numbervmailmessages','totaldayminutes','totaldaycalls','totaldaycharge','totaleveminutes','totalevecalls','totalevecharge','totalnightminutes','totalnightcalls','totalnightcharge','totalintlminutes','totalintlcalls','totalintlcharge','numbercustomerservicecalls']
categorical_features = ['internationalplan','voicemailplan']

df1[numerical_features].describe().transpose()

df1[numerical_features].hist(bins=30, figsize=(10, 7))

# Plot with respect to total charges
df = df1
fig, ax = plt.subplots(3, 5, figsize=(14, 4))
df[df.churn == "No"][numerical_features].hist(bins=30, color="blue", alpha=0.5, ax=ax)
df[df.churn == "Yes"][numerical_features].hist(bins=30, color="red", alpha=0.5, ax=ax)

fig, ax = plt.subplots(1, 2, figsize=(12, 9))
categorical_features = ['internationalplan','voicemailplan']
df['internationalplan'].value_counts().plot('bar',ax=ax[0]).set_title('internationalplan')
df['voicemailplan'].value_counts().plot('bar',ax=ax[1]).set_title('voicemailplan')

feature = 'internationalplan'
fig, ax = plt.subplots(1, 2, figsize=(14, 4))
df[df.churn == "No"][feature].value_counts().plot('bar', ax=ax[0]).set_title('not churned customers with internationalplan')
df[df.churn == "Yes"][feature].value_counts().plot('bar', ax=ax[1]).set_title('churned customers with internationalplan')

feature = 'voicemailplan'
fig, ax = plt.subplots(1, 2, figsize=(14, 4))
df[df.churn == "No"][feature].value_counts().plot('bar', ax=ax[0]).set_title('not churned customers with voicemailplan')
df[df.churn == "Yes"][feature].value_counts().plot('bar', ax=ax[1]).set_title('churned customers with voicemailplan')

df[target].value_counts().plot('bar').set_title('churned')

# Convert Categorical Features to Numeric features using encoding or binary classification

df[target].unique()

df[target].value_counts()

df.churn = df.churn.map(dict(Yes=1, No=0))

df[target].value_counts()

"""Convert other two categorical **variables**"""

df[categorical_features[0]].value_counts()

df[categorical_features[1]].value_counts()

categorical_features = ['internationalplan','voicemailplan']
df.internationalplan = df.internationalplan.map(dict(yes=1, no=0))
df['internationalplan'].value_counts()

df.voicemailplan = df.voicemailplan.map(dict(yes=1, no=0))
df['voicemailplan'].value_counts()

df.transpose()

# Apply Machine Learning Model
# Convert data from pandas dataframe to numpy array
XY = df.to_numpy()
np.set_printoptions(suppress=True)
XY[1,:]

XY.shape

X = XY[:,1:18]
y = XY[:,0]

X[1,:]

y[1]

#Scale the inputs
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X = scaler.fit_transform(X)

X[1,:]

print("Feature space holds %d observations and %d features" %(X.shape))
print("Unique target labels:",np.unique(y))

sum(y)

# Import required libraries
import pandas as pd
import numpy as np 
import matplotlib.pyplot as plt
import sklearn

# Import necessary modules
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from math import sqrt
from sklearn import model_selection
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import KFold
from sklearn.model_selection import LeaveOneOut
from sklearn.model_selection import LeavePOut
from sklearn.model_selection import ShuffleSplit
from sklearn.model_selection import StratifiedKFold

# Evaluate using a train and a test set
X_train, X_test, Y_train, Y_test = model_selection.train_test_split(X, y, test_size=0.30, random_state=100)
model_log = LogisticRegression()
model_log.fit(X_train, Y_train)
result = model_log.score(X_test, Y_test)
print("\nAccuracy: %.2f%%" % (result*100.0))

# Two Examples of prediction
# 0,141,1,1,37,258.6,84,43.96,222.0,111,18.87,326.4,97,14.69,11.2,5,3.02,0
# 1,65,0,0,0,129.1,137,21.95,228.5,83,19.42,208.8,111,9.4,12.7,6,3.43,4
X1 = np.array([141,1,1,37,258.6,84,43.96,222.0,111,18.87,326.4,97,14.69,11.2,5,3.02,0])
X1 = X1.reshape(1,-1)
X1 = scaler.transform(X1)
y1_pred = model_log.predict(X1)
print("Predicted value is: ",y1_pred)
X1 = np.array([65,0,0,0,129.1,137,21.95,228.5,83,19.42,208.8,111,9.4,12.7,6,3.43,4])
X1 = X1.reshape(1,-1)
X1 = scaler.transform(X1)
y1_pred = model_log.predict(X1)
print("Predicted value is: ",y1_pred)
X2 = XY[:,1:18]
X2 = scaler.transform(X2)

y2_pred = model_log.predict(X2)
i = 0
for i in range(len(y2_pred)):
  if(y2_pred[i] == 0):
    print("index: ",i, "value 0:")
for i in range(len(y2_pred)):
  if(y2_pred[i] == 1):
    print("index: ",i, "value 1:")

"""**Below is left as an exercise**"""

from sklearn.model_selection import KFold

def run_cv(X,y,clf_class,**kwargs):
    # Construct a kfolds object
    kf = KFold(n_splits=5,shuffle=True,random_state=None)
    y_pred = y.copy()
    #print("kf.split(): ",kf.split(X), "\n")
    i = 0

    # Iterate through folds
    for train_index, test_index in kf.split(X):
        #print("train index: ",train_index," ,test index",test_index,"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n")
        X_train, X_test = X[train_index], X[test_index]
        y_train = y[train_index]
        # Initialize a classifier with key word arguments
        clf = clf_class(**kwargs)
        clf.fit(X_train,y_train)
        y_pred[test_index] = clf.predict(X_test)
        print("i : ",i)
        i = i+1 
    return y_pred

import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier as RF
from sklearn.neighbors import KNeighborsClassifier as KNN

def accuracy(y_true,y_pred):
    # NumPy interprets True and False as 1. and 0.
    return np.mean(y_true == y_pred)

print("Accuracy score for Support vector machines:")
y_pred = run_cv(X,y,SVC)
print("total predicted yes: ",np.sum(y_pred),"\n")
print("%.3f" % accuracy(y, y_pred))
print("Accuracy score for Random forest:")
y_pred = run_cv(X,y,RF)
print("total predicted yes: ",np.sum(y_pred),"\n")
print("%.3f" % accuracy(y, y_pred))
print("Accuracy score for K-nearest-neighbors:")
y_pred = run_cv(X,y,KNN)
print("total predicted yes: ",np.sum(y_pred),"\n")
print("%.3f" % accuracy(y, y_pred))

"""Precision and recall
Measurements aren't golden formulas which always spit out high numbers for good models and low numbers for bad ones. Inherently they convey something sentiment about a model's performance, and it's the job of the human designer to determine each number's validity. The problem with accuracy is that outcomes aren't necessarily equal. If my classifier predicted a customer would churn and they didn't, that's not the best but it's forgivable. However, if my classifier predicted a customer would return, I didn't act, and then they churned... that's really bad.

I'll be using another built in scikit-learn function to construction a confusion matrix. A confusion matrix is a way of visualizing predictions made by a classifier and is just a table showing the distribution of predictions for a specific class. The x-axis indicates the true class of each observation (if a customer churned or not) while the y-axis corresponds to the class predicted by the model (if my classifier said a customer would churned or not).
"""

# Python script for confusion matrix creation. 
# Source: https://www.geeksforgeeks.org/confusion-matrix-machine-learning/
#              predicted  predicted 
#  true label 
#  true label 
from sklearn.metrics import confusion_matrix 
from sklearn.metrics import accuracy_score 
from sklearn.metrics import classification_report 
actual = y
predicted = run_cv(X,y,SVC)
results = confusion_matrix(actual,predicted) 
print('Support Vector Machine Confusion Matrix :')
print(results) 
print('Accuracy Score :',accuracy_score(y, predicted)) 
print('Report : ')
print(classification_report(actual, predicted))

!pip install --upgrade scikit-learn

import numpy as np
import matplotlib.pyplot as plt

from sklearn import svm, datasets
from sklearn.model_selection import train_test_split
from sklearn.metrics import plot_confusion_matrix
classifier = svm.SVC().fit(X, y)
# Plot non-normalized confusion matrix
titles_options = [("Confusion matrix, without normalization", None),
                  ("Normalized confusion matrix", 'true')]
for title, normalize in titles_options:
    disp = plot_confusion_matrix(classifier, X, y,
                                 display_labels=['no','yes'],
                                 cmap=plt.cm.Blues,
                                 normalize=normalize)
    disp.ax_.set_title(title)

    print(title)
    print(disp.confusion_matrix)

plt.show()

# Two Examples of prediction
# 0,141,1,1,37,258.6,84,43.96,222.0,111,18.87,326.4,97,14.69,11.2,5,3.02,0
# 1,65,0,0,0,129.1,137,21.95,228.5,83,19.42,208.8,111,9.4,12.7,6,3.43,4
X = np.array([135,1,0,30,259.6,90,49,200.0,100,18,300,100,19,14,5,7,2])
X = X.reshape(1,-1)
print("Predicted value is: ",y_pred = run_cv(X,y,RF))

X = np.array([65,0,0,0,129.1,137,21.95,228.5,83,19.42,208.8,111,9.4,12.7,6,3.43,4])
X = X.reshape(1,-1)
X = XY[:,1:18]
print("Predicted value is: ",classifier.predict(X))

